{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders with Keras, TensorFlow, and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://pyimagesearch.com/wp-content/uploads/2020/02/keras_autoencoders_header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we’ll discuss what autoencoders are, including how convolutional autoencoders can be applied to image data. We’ll also discuss the difference between autoencoders and other generative models, such as Generative Adversarial Networks (GANs).\n",
    "\n",
    "From there, I’ll show you how to implement and train a convolutional autoencoder using Keras and TensorFlow.\n",
    "\n",
    "Finally, we’ll then review the results of the training script, including visualizing how the autoencoder did at reconstructing the input data.\n",
    "\n",
    "I’ll recommend next steps to you if you are interested in learning more about deep learning applied to image datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are autoencoders?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are a type of unsupervised neural network (i.e., no class labels or labeled data) that seek to:\n",
    "\n",
    "Accept an input set of data (i.e., the input).\n",
    "Internally compress the input data into a latent-space representation (i.e., a single vector that compresses and quantifies the input).\n",
    "Reconstruct the input data from this latent representation (i.e., the output).\n",
    "Typically, we think of an autoencoder having two components/subnetworks:\n",
    "\n",
    "Encoder: Accepts the input data and compresses it into the latent-space. If we denote our input data as x and the encoder as E, then the output latent-space representation, s, would be $s = E(x)$.\n",
    "Decoder: The decoder is responsible for accepting the latent-space representation s and then reconstructing the original input. If we denote the decoder function as D and the output of the detector as o, then we can represent the decoder as $o = D(s)$.\n",
    "Using our mathematical notation, the entire training process of the autoencoder can be written as:\n",
    "\n",
    "$$o = D(E(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://pyimagesearch.com/wp-content/uploads/2020/02/keras_autoencoder_arch_flow.png)\n",
    "\n",
    "Figure 1: Autoencoders with Keras, TensorFlow, Python, and Deep Learning don’t have to be complex. Breaking the concept down to its parts, you’ll have an input image that is passed through the autoencoder which results in a similar output image. (figure inspired by Nathan Hubens’ article, Deep inside: Autoencoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see that:\n",
    "\n",
    "We input a digit to the autoencoder.\n",
    "The encoder subnetwork creates a latent representation of the digit. This latent representation is substantially smaller (in terms of dimensionality) than the input.\n",
    "The decoder subnetwork then reconstructs the original digit from the latent representation.\n",
    "You can thus think of an autoencoder as a network that reconstructs its input!\n",
    "\n",
    "To train an autoencoder, we input our data, attempt to reconstruct it, and then minimize the mean squared error (or similar loss function).\n",
    "\n",
    "Ideally, the output of the autoencoder will be near identical to the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process, our goal is to train a network that can learn how to reconstruct our input data — but the true value of the autoencoder lives inside that latent-space representation.\n",
    "\n",
    "Keep in mind that autoencoders compress our input data and, more to the point, when we train autoencoders, what we really care about is the encoder, E, and the latent-space representation, $s = E(x)$.\n",
    "\n",
    "The decoder, $o = D(s)$, is used to train the autoencoder end-to-end, but in practical applications, we often (but not always) care more about the encoder and the latent-space.\n",
    "\n",
    "Later in this tutorial, we’ll be training an autoencoder on the MNIST dataset. The MNIST dataset consists of digits that are 28×28 pixels with a single channel, implying that each digit is represented by 28 x 28 = 784 values. The autoencoder we’ll be training here will be able to compress those digits into a vector of only 16 values — that’s a reduction of nearly 98%!\n",
    "\n",
    "So what can we do if an input data point is compressed into such a small vector?\n",
    "\n",
    "That’s where things get really interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are applications of autoencoders?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://pyimagesearch.com/wp-content/uploads/2020/02/keras_autoencoders_applications.png)\n",
    "\n",
    "Autoencoders are typically used for dimensionality reduction, denoising, and anomaly/outlier detection. Outside of computer vision, they are extremely useful for Natural Language Processing (NLP) and text comprehension. In this tutorial, we’ll use Python and Keras/TensorFlow to train a deep learning autoencoder. (image source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are typically used for:\n",
    "\n",
    "Dimensionality reduction (i.e., think PCA but more powerful/intelligent).\n",
    "Denoising (ex., removing noise and preprocessing images to improve OCR accuracy).\n",
    "Anomaly/outlier detection (ex., detecting mislabeled data points in a dataset or detecting when an input data point falls well outside our typical data distribution).\n",
    "Outside of the computer vision field, you’ll see autoencoders applied to Natural Language Processing (NLP) and text comprehension problems, including understanding the semantic meaning of words, constructing word embeddings, and even text summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ tree --dirsfirst\n",
    ".\n",
    "├── pyimagesearch\n",
    "│   ├── __init__.py\n",
    "│   └── convautoencoder.py\n",
    "├── output.png\n",
    "├── plot.png\n",
    "└── train_conv_autoencoder.py\n",
    "1 directory, 5 files\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, depth = 28, 28, 1\n",
    "filters=(32, 64)\n",
    "latentDim=16\n",
    "\n",
    "\n",
    "inputShape = (height, width, depth)\n",
    "chanDim = -1\n",
    "\n",
    "# define the input to the encoder\n",
    "inputs = Input(shape=inputShape)\n",
    "x = inputs\n",
    "# loop over the number of filters\n",
    "for f in filters:\n",
    "    # apply a CONV => RELU => BN operation\n",
    "    x = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "    x = BatchNormalization(axis=chanDim)(x)\n",
    "# flatten the network and then construct our latent vector\n",
    "volumeSize = K.int_shape(x)\n",
    "x = Flatten()(x)\n",
    "latent = Dense(latentDim)(x)\n",
    "# build the encoder model\n",
    "encoder = Model(inputs, latent, name=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 14, 14, 32)        320       \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 14, 14, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 14, 14, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 7, 7, 64)          18496     \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 7, 7, 64)         256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                50192     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 69,392\n",
      "Trainable params: 69,200\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "latentInputs = Input(shape=(latentDim,))\n",
    "x = Dense(np.prod(volumeSize[1:]))(latentInputs)  # !!!!!!!!!!!!!!!\n",
    "x = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
    "# loop over our number of filters again, but this time in\n",
    "# reverse order\n",
    "for f in filters[::-1]:\n",
    "\t# apply a CONV_TRANSPOSE => RELU => BN operation\n",
    "\tx = Conv2DTranspose(f, (3, 3), strides=2,\n",
    "\t\tpadding=\"same\")(x)\n",
    "\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "# apply a single CONV_TRANSPOSE layer used to recover the\n",
    "# original depth of the image\n",
    "x = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
    "outputs = Activation(\"sigmoid\")(x)\n",
    "# build the decoder model\n",
    "decoder = Model(latentInputs, outputs, name=\"decoder\")\n",
    "# our autoencoder is the encoder + decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3136)              53312     \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 14, 14, 64)       36928     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 14, 14, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 32)       18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 28, 28, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 28, 28, 32)       128       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        289       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,377\n",
      "Trainable params: 109,185\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " encoder (Functional)        (None, 16)                69392     \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 28, 28, 1)         109377    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 178,769\n",
      "Trainable params: 178,385\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = Model(inputs, decoder(encoder(inputs)),\n",
    "\t\t\tname=\"autoencoder\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CLASE CON EL AUTOENCODER\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class ConvAutoencoder:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, filters=(32, 64), latentDim=16):\n",
    "\t\t# initialize the input shape to be \"channels last\" along with\n",
    "\t\t# the channels dimension itself\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# ENCODER\n",
    "\n",
    "        # define the input to the encoder\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = inputs\n",
    "\t\t# loop over the number of filters\n",
    "\t\tfor f in filters:\n",
    "\t\t\t# apply a CONV => RELU => BN operation\n",
    "\t\t\tx = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\t# flatten the network and then construct our latent vector\n",
    "\t\tvolumeSize = K.int_shape(x) # !!!!!!!!!!!!!!!\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tlatent = Dense(latentDim)(x)\n",
    "\t\t# build the encoder model\n",
    "\t\tencoder = Model(inputs, latent, name=\"encoder\")\n",
    "\n",
    "\t\t# DECODER\n",
    "\n",
    "\t\t# start building the decoder model which will accept the\n",
    "\t\t# output of the encoder as its inputs\n",
    "\t\tlatentInputs = Input(shape=(latentDim,))\n",
    "\t\tx = Dense(np.prod(volumeSize[1:]))(latentInputs)  # !!!!!!!!!!!!!!!\n",
    "\t\tx = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
    "\t\t# loop over our number of filters again, but this time in\n",
    "\t\t# reverse order\n",
    "\t\tfor f in filters[::-1]:\n",
    "\t\t\t# apply a CONV_TRANSPOSE => RELU => BN operation\n",
    "\t\t\tx = Conv2DTranspose(f, (3, 3), strides=2,\n",
    "\t\t\t\tpadding=\"same\")(x)\n",
    "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\n",
    "\t\t# apply a single CONV_TRANSPOSE layer used to recover the\n",
    "\t\t# original depth of the image\n",
    "\t\tx = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
    "\t\toutputs = Activation(\"sigmoid\")(x)\n",
    "\t\t# build the decoder model\n",
    "\t\tdecoder = Model(latentInputs, outputs, name=\"decoder\")\n",
    "\t\t# our autoencoder is the encoder + decoder\n",
    "\t\tautoencoder = Model(inputs, decoder(encoder(inputs)),\n",
    "\t\t\tname=\"autoencoder\")\n",
    "\t\t# return a 3-tuple of the encoder, decoder, and autoencoder\n",
    "\t\treturn (encoder, decoder, autoencoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "\n",
    "# import the necessary packages\n",
    "# from pyimagesearch.convautoencoder import ConvAutoencoder\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import argparse\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 8\n",
    "output = \"output.png\"\n",
    "plot = \"plot.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset...\n"
     ]
    }
   ],
   "source": [
    "# initialize the number of epochs to train for and batch size\n",
    "EPOCHS = 5\n",
    "BS = 32\n",
    "# load the MNIST dataset\n",
    "print(\"[INFO] loading MNIST dataset...\")\n",
    "((trainX, _), (testX, _)) = mnist.load_data()\n",
    "# add a channel dimension to every image in the dataset, then scale\n",
    "# the pixel intensities to the range [0, 1]\n",
    "trainX = np.expand_dims(trainX, axis=-1)\n",
    "testX = np.expand_dims(testX, axis=-1)\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building autoencoder...\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 15s 7ms/step - loss: 0.0188 - val_loss: 0.0109\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0103 - val_loss: 0.0094\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0093 - val_loss: 0.0089\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0087 - val_loss: 0.0083\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0083 - val_loss: 0.0079\n"
     ]
    }
   ],
   "source": [
    "# construct our convolutional autoencoder\n",
    "print(\"[INFO] building autoencoder...\")\n",
    "(encoder, decoder, autoencoder) = ConvAutoencoder.build(28, 28, 1)\n",
    "opt = Adam(learning_rate=1e-3)\n",
    "autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
    "# train the convolutional autoencoder\n",
    "H = autoencoder.fit(\n",
    "\ttrainX, trainX,\n",
    "\tvalidation_data=(testX, testX),\n",
    "\tepochs=EPOCHS,\n",
    "\tbatch_size=BS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a plot that plots and saves the training history\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] making predictions...\n",
      "313/313 [==============================] - 1s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the convolutional autoencoder to make predictions on the\n",
    "# testing images, then initialize our list of output images\n",
    "print(\"[INFO] making predictions...\")\n",
    "decoded = autoencoder.predict(testX)\n",
    "outputs = None\n",
    "# loop over our number of output samples\n",
    "for i in range(0, samples):\n",
    "\t# grab the original image and reconstructed image\n",
    "\toriginal = (testX[i] * 255).astype(\"uint8\")\n",
    "\trecon = (decoded[i] * 255).astype(\"uint8\")\n",
    "\t# stack the original and reconstructed image side-by-side\n",
    "\toutput = np.hstack([original, recon])\n",
    "\t# if the outputs array is empty, initialize it as the current\n",
    "\t# side-by-side image display\n",
    "\tif outputs is None:\n",
    "\t\toutputs = output\n",
    "\t# otherwise, vertically stack the outputs\n",
    "\telse:\n",
    "\t\toutputs = np.vstack([outputs, output])\n",
    "# save the outputs image to disk\n",
    "cv2.imwrite(\"PREDICTIONS.png\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('basic_ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70dd3549dabd1ce49e5001f72f872ceffdb9455a9d169ec133c29fbc27295603"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
